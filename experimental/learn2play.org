* L2P Hearthstone
** Part 1
*** Hearthstone or Not?

    The objective is to take videos and classify whether they are Hearthstone
    or not.

**** Annotating
     :PROPERTIES:
     :header-args: :tangle hs_annotate.py :comments both :noweb yes
     :END:

     The first step is necessarily annotation. I have a bunch of videos and
     need to note the exact frames at which the transition from Hearthstone to
     Not (or visa-versa) occurs.


***** Annotation Tool
      This is a tool to annotate videos with binary labels.

      #+begin_src python
import numpy as np
import cv2
import sys
from functools import partial
     #+end_src

      #+name: load_video
      #+begin_src python
def load_video(filename):
    cap = cv2.VideoCapture(filename)
    return cap, current_frame(cap), num_frames(cap)
     #+end_src

      #+name: meta
      #+begin_src python
def num_frames(cap):
    return int(cap.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT))

def current_frame(cap):
    return int(cap.get(cv2.cv.CV_CAP_PROP_POS_FRAMES))
     #+end_src

      #+name: seek
      #+begin_src python
def seek(cap, pos):
    if pos > num_frames(cap):
        raise Exception

    cap.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, pos)
     #+end_src

      #+begin_src python
def main():
    cap, position, max_position = load_video(sys.argv[1])

    window_name = 'frame'  # winning an award for 'most creative name' for sure
    cv2.namedWindow(window_name)
    cv2.createTrackbar('frame_bar', window_name,
                       position, max_position,
                       partial(seek, cap))

    partition_points = []
    advance = False
    save = True

    ret, frame = cap.read()

    try:
        while cap.isOpened() and current_frame(cap) < num_frames(cap):
            if advance:
                ret, frame = cap.read()
                cv2.setTrackbarPos('frame_bar', window_name, current_frame(cap))

            cv2.imshow(window_name, frame)

            key = cv2.waitKey(25) & 0xFF

            if key == 27:  # ESC
                save = False
                break
            elif key == ord('.'):
                ret, frame = cap.read()
                cv2.setTrackbarPos('frame_bar', window_name, current_frame(cap))
            elif not advance and key == ord('t'):
                index = current_frame(cap)
                if index in partition_points:
                    partition_points.remove(index)
                else:
                    partition_points.append(index)
                    partition_points.sort()
                    print(partition_points)
            elif key == ord(' '):
                advance = not advance

        cap.release()
        cv2.destroyAllWindows()

    except Exception as e:
        print(e)

    finally:
        if save:
            with open(sys.argv[1] + '.partitions', 'w') as dest:
                dest.write(','.join(map(str, partition_points)))

if __name__ == "__main__":
    main()
     #+end_src
**** Classifying

***** Extracting Frames

      Gonna use a generator for frames from a video because =cv2= doesn't have
      a nice iterable for them.

      #+name: frame-gen
      #+begin_src python
def frames(cap):
    while True:
        ret, frame = cap.read()
        if ret:
            yield frame
        else:
            break
      #+end_src

      This function creates the cv2 cap object as well as the list of partition points.
      #+name: cap-open
      #+begin_src python
def cap_open(path):
    with open(path + '.partitions') as f:
        return cv2.VideoCapture(path), f.read().split(',')
      #+end_src

***** Extracting Features
****** Base Class

       I want to be able to easily add new features. Building a base Luigi task
       for computing features makes sense. The contract for extending this class is
       very simple:

       - =identifier(self) -> str=
         Generates a string identifying the type of features being computed.
       - =features(self, frame) -> np.array=
         Generates a set of features for a frame.

       #+name: task:features
       #+begin_src python
class Features(luigi.Task):
    video = luigi.Parameter()
    rate = luigi.IntParameter(default=10)  # 1 in $rate frames are used
    color = luigi.BooleanParameter(default=True)

    def identifier(self):
        raise ValueError("Do not use the base class!")

    def output(self, type=None):
        path = self.video + '_{type}_{gray}{rate}.npz'.format(
            rate=self.rate,
            type=self.identifier(),
            gray=('gray_' if not self.color else '')
        )
        return luigi.LocalTarget(path)

    def features(self):
        raise ValueError("Do not use the base class!")

    def run(self):
        <<frame-gen>>

        cap = cv2.VideoCapture(self.video)

        video_features = {str(index): self.features(frame)
                          for index, frame in enumerate(frames(cap))
                          if index % self.rate == 0}
        # some frames produce None for their descriptor, don't know why
        video_features = {k: v for k, v in video_features.items()
                          if v is not None}

        with self.output().open('w') as out:
            np.savez_compressed(out, **video_features)
       #+end_src

****** Specifying a Feature Type

       To specify the kind of features for a task in a variable fashion, simply
       add a =TaskParameter()=, which will return a sub-class of =Features=
       or throw an error.

       #+name: param:features
       #+begin_src python
class TaskParameter(luigi.Parameter):
    def parse(self, x):
        if type(x) == str:
            cls = None
            for key, module in sys.modules.items():
                if hasattr(module, x):
                    cls = getattr(module, x)
                    if issubclass(cls, luigi.Task):
                        break
                    else:
                        cls = None
        else:
            cls = x

        if cls is not None and issubclass(cls, luigi.Task) and not cls == luigi.Task:
            return cls
        else:
            raise ValueError('Unknown task requested.')
      #+end_src

****** SURF Features
       SURF feature implementation using =cv2.SURF=.

       #+name: features:surf
       #+begin_src python
class SURFFeatures(Features):
    hessianThreshold = luigi.IntParameter(default=5000)
    upright = luigi.BooleanParameter(default=True)

    def identifier(self):
        return 'surf_{upright}{hessian}'.format(
            upright='upright_' if self.upright else '',
            hessian=self.hessianThreshold
        )

    def surf(self):
        return cv2.SURF(upright=self.upright,
                        hessianThreshold=self.hessianThreshold)

    def features(self, frame):
        kp, desc = self.surf().detectAndCompute(frame, None)
        return desc
       #+end_src

****** SURF Histograms

       SURF BoVW histograms. Has a forward dependency on the =Vocabulary= task.

       #+name: features:surf_histogram
       #+begin_src python
class SURFHistograms(SURFFeatures):

    def requires(self):
        return {
            'vocab': Vocabulary(
                rate=self.rate,
                features=SURFFeatures
            ),
            'surf': SURFFeatures(
                rate=self.rate,
                video=self.video
            )
        }

    def identifier(self):
        return 'surf_hist_{upright}{hessian}'.format(
            upright='upright_' if self.upright else '',
            hessian=self.hessianThreshold
        )

    def run(self):
        centers = np.load(self.input()['vocab'].path)['centers']
        features = np.load(self.input()['surf'].path)

        def histogram(feat):
            h = np.zeros((len(centers),), dtype='float32')
            for f in feat:
                dist = np.linalg.norm(centers - f, axis=1)
                h[dist.argmax()] += 1
            return h / np.linalg.norm(h)

        with self.output().open('w') as f:
            np.savez_compressed(
                f,
                ,**{k: histogram(v)
                   for k, v in features.iteritems()})
       #+end_src
****** Tiny Images

       Simple Tiny Images implementation: resize the image to be really
       fragging small.

       #+name: features:tiny
       #+begin_src python
class TinyFeatures(Features):
    size = luigi.Parameter(default='16x16')

    def identifier(self):
        return 'tiny_{size}'.format(size=self.size)

    def features(self, frame):
        from scipy.misc import imresize

        size = map(int, self.size.split('x'))
        return imresize(frame, size).reshape(-1)
      #+end_src
****** All Features Combined

       #+name: all_features
       #+begin_src python
<<task:features>>


<<param:features>>


<<task:vocabulary>>


<<features:surf>>


<<features:surf_histogram>>


<<features:tiny>>
       #+end_src

***** Building Vocabulary

      I am using a Bag of Visual Words model with k-means clustering to develop
      a vocabulary. The =Vocabulary= task encodes the construction of a
      vocabulary on a set of videos with a number of words.

      #+name: task:vocabulary
      #+begin_src python
class Vocabulary(luigi.Task):
    num_words = luigi.IntParameter(default=300)
    num_videos = luigi.IntParameter(default=10)
    rate = luigi.IntParameter(default=100)
    seed = luigi.IntParameter(default=None, significant=False)
    features = TaskParameter()

    def output(self):
        path = 'training/{ident}_{words}_{videos}.npz'.format(
            ident=self.features(video='', rate=self.rate).identifier(),
            words=self.num_words,
            videos=self.num_videos
        )
        return luigi.LocalTarget(path)

    def run(self):
        from glob import glob
        from random import seed, shuffle
        paths = glob('twitch/Hearthstone/*/*/raws/*.flv')
        seed(self.seed)
        shuffle(paths)

        all_features = []
        feats = yield [self.features(video=path,rate=self.rate)
                       for path in paths[:self.num_videos]]

        for feat in feats:
            with feat.open('r') as f:
                features = dict(np.load(f).items()).values()
                all_features += features

        all_features = np.vstack(all_features)

        compactness, labels, centers = cv2.kmeans(
            all_features, self.num_words,
            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,1000,1.0),
            10, 0)

        with self.output().open('w') as out:
            np.savez_compressed(
                out,
                compactness=compactness,
                labels=labels,
                centers=centers
            )
      #+end_src

***** Constructing the SVM Model

      #+name: task:svm-hs
      #+begin_src python

class Dataset(luigi.Task):
    rate = luigi.IntParameter(default=100)
    features = TaskParameter()
    all_videos = luigi.BooleanParameter(default=False)

    def known_paths(self):
        path = 'twitch/Hearthstone/*/*/raws/*.flv'
        if not self.all_videos:
            path += '.partitions'
        return glob(path)

    def response(self, partitions, frame_num):
        hearthstone = False
        for partition in partitions:
            if frame_num < partition:
                return (1 if hearthstone else -1)
            hearthstone = not hearthstone
        return (1 if hearthstone else -1)


    def load_train_test(self, paths, features):
        trainData = []
        responses = []
        for path, feat in zip(paths, features):
            with open(path, 'r') as f:
                p_raw = f.read()
                if p_raw:
                    partitions = [int(p) for p in p_raw.split(',')]
                else:
                    partitions = []

            with feat.open('r') as f:
                features = np.load(f).items()
                for frame_num, feat in features:
                    frame_num = int(frame_num)
                    resp = self.response(partitions, frame_num)
                    trainData.append(feat)
                    responses.append(resp)

        trainData = np.vstack(trainData)
        responses = np.vstack(responses)
        return trainData, responses

    def requires(self):
        paths = self.known_paths()
        return [self.features(video=path.rstrip('.partitions'),
                              rate=self.rate)
                for path in paths]

    def output(self):
        path = 'datasets/{id}_{rate}.npz'.format(
            id=self.features(video='', rate=self.rate).identifier(),
            rate=self.rate
        )
        return luigi.LocalTarget(path)

    def run(self):
        train, resp = self.load_train_test(self.known_paths(), self.input())

        with self.output().open('w') as f:
            np.savez_compressed(f,
                                trainData=train,
                                responses=resp)

class HearthstoneModel(luigi.Task):
    rate = luigi.IntParameter(default=100)
    features = TaskParameter()
    rand_state = luigi.IntParameter(default=None)
    test_size = luigi.FloatParameter(default=0.5)

    def __init__(self, *args, **kwargs):
        from random import randint
        super(HearthstoneModel, self).__init__(*args, **kwargs)
        if self.rand_state is None:
            self.rand_state = random.randint(0, np.iinfo(np.uint32).max)

    def requires(self):
        return Dataset(rate=self.rate, features=self.features)

    def output(self):
        return luigi.LocalTarget('training/hearthstone_model_{ident}.jl'.format(
            ident=self.features(video='', rate=self.rate).identifier()
        ))

    def run(self):
        from sklearn import svm
        from sklearn.cross_validation import train_test_split
        from sklearn.externals import joblib
        dataset = np.load(self.input().path)

        f_train, _, r_train, _ = train_test_split(
            dataset['trainData'], dataset['responses'],
            test_size=self.test_size,
            random_state=self.rand_state
        )

        clf = svm.LinearSVC()
        clf.fit(
            dataset['trainData'],
            dataset['responses'],
        )

        joblib.dump(clf, self.output().path)
      #+end_src
***** Evaluation
      An ROC curve makes sense for evaluating the performance of this
      method. The following task computes one, producing a PNG plot with the
      AUC on it.

      #+name: task:roc
      #+begin_src python
class ROCCurve(luigi.Task):
    model = TaskParameter()
    rate = luigi.IntParameter()
    features = TaskParameter()
    rand_state = luigi.IntParameter(default=None)
    test_size = luigi.FloatParameter(default=0.5)

    def __init__(self, *args, **kwargs):
        from random import randint
        super(ROCCurve, self).__init__(*args, **kwargs)
        if self.rand_state is None:
            self.rand_state = random.randint(0, np.iinfo(np.uint32).max)

    def requires(self):
        return {
            'model': self.model(
                rand_state=self.rand_state,
                test_size=self.test_size,
                features=self.features,
                rate=self.rate
            ),
            'dataset': Dataset(rate=self.rate,
                               features=self.features)
        }

    def output(self):
        self.model_name = os.path.splitext(
            os.path.basename(self.input()['model'].path))[0]

        return luigi.LocalTarget('plots/roc_{rate}_{model_name}.png'.format(
            model_name=self.model_name,
            rate=self.rate
        ))

    def run(self):
        from sklearn.cross_validation import train_test_split
        from sklearn.externals import joblib
        from sklearn.metrics import roc_curve, auc

        dataset = np.load(self.input()['dataset'].path)

        _, f_test, _, r_test = train_test_split(
            dataset['trainData'], dataset['responses'],
            test_size=self.test_size,
            random_state=self.rand_state
        )

        classifier = joblib.load(self.input()['model'].path)

        r_scores = classifier.predict(f_test)

        fpr, tpr, _ = roc_curve(r_test, r_scores)
        auc = auc(fpr, tpr)

        plt.figure()
        plt.plot(fpr, tpr)
        plt.xlabel('False Positive Rate')
        plt.xlim([0.0, 1.0])
        plt.ylabel('True Positive Rate')
        plt.ylim([0.0, 1.05])
        plt.title("ROC for {model_name} (Test size: {ts}, AUC: {auc})".format(
            model_name=self.model_name,
            ts=self.test_size,
            auc=auc
        ))
        try:
            os.makedirs(os.path.dirname(self.output().path))
        except:
            pass

        plt.savefig(self.output().path)
      #+end_src

***** Putting it Together

      #+begin_src python :tangle learn2play/hearthstone_classifier.py :comments both :noweb yes
import luigi
import cPickle as pickle
import cv2
import sklearn
import numpy as np
import random
from matplotlib import pyplot as plt
import os
import sys
from glob import glob, iglob


<<all_features>>


<<task:svm-hs>>

<<task:roc>>

if __name__ == "__main__":
    luigi.run(local_scheduler=True)
      #+end_src

***** Results

      Results are computed as the AUC of ROC curves with a 50/50 train/test
      split. Only 6 videos were annotated.

****** SURF Histogram Features
       These features are computed with =hessianThreshold=5000=, a Bag of
       Visual Words with =n=200= and a sampling rate of =1/1000=.

       #+name: roc:svm+surf
       #+begin_src python :results value file :exports results
import luigi
import sys
from luigi.scheduler import CentralPlannerScheduler as CSP
from luigi.worker import Worker
from learn2play.hearthstone_classifier import ROCCurve, HearthstoneModel, SURFHistograms

roc = ROCCurve(
    rate=1000,
    model=HearthstoneModel,
    features=SURFHistograms
)

w = Worker(scheduler=CSP())
w.add(roc)
if w.run():
    return roc.output().path
       #+end_src

       #+RESULTS: roc:svm+surf
       [[file:plots/roc_1000_hearthstone_model_surf_hist_upright_5000.png]]

       Not as good as I'd hoped, especially given how long it takes to compute
       SURF Features over that many frames.

       *AUC:* 0.846

****** Tiny Images

       Tiny Images seem like a good fit for Hearthstone games, given that
       rotation and position within the image are pretty constant.

       #+name: roc:svm+tiny
       #+begin_src python :results value file :exports results
import luigi
import sys
from luigi.scheduler import CentralPlannerScheduler as CSP
from luigi.worker import Worker
from learn2play import ROCCurve, HearthstoneModel, TinyFeatures

roc = ROCCurve(
    rate=100,
    model=HearthstoneModel,
    features=TinyFeatures
)

w = Worker(scheduler=CSP())
w.add(roc)
if w.run():
    return roc.output().path
       #+end_src

       #+RESULTS: roc:svm+tiny
       [[file:plots/roc_100_hearthstone_model_tiny_16x16.png]]

       Turns out that that intuition was spot-on.

       *AUC*: 1.0

****** Secondary Evaluation

       Since the Tiny Images feature set performed so incredibly well (too
       well, some might say), I decided to give it a secondary semi-manual
       evaluation. Here is a small script which displays a video with an
       overlay of whether it is =Hearthstone= or =Not Hearthstone=.
       #+begin_src python :tangle part1_eval.py :noweb yes :exports none
import cv2
from learn2play.hearthstone_classifier import *
from sys import argv
from sklearn.externals import joblib

<<frame-gen>>

model_task = HearthstoneModel(rate=1000, features=TinyFeatures)
classifier = joblib.load(model_task.output().path)

tinyfy = TinyFeatures(video='', rate=1000).features

cap = cv2.VideoCapture(argv[1])

print(argv[1])

def textify(frame, text):
    copy = frame.copy()
    cv2.putText(copy, text, (10,100), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)
    return copy

cv2.namedWindow('frame')
for frame in frames(cap):
    tiny = tinyfy(frame)
    prediction = classifier.predict([tiny])
    if prediction == 1:
        cv2.imshow('frame', textify(frame, 'Hearthstone'))
    else:
        cv2.imshow('frame', textify(frame, 'Not Hearthstone'))
    cv2.waitKey(10)
cv2.destroyAllWindows()
       #+end_src

       #+begin_src sh
       python part1_eval.py twitch/Hearthstone/13470812000/c3673800/raws/live_user_morikcm_1391579178.flv
       #+end_src

       This secondary evaluation revealed that dataset bias is a factor in the
       accuracy of tiny images. For example, the following frame is (wrongly)
       marked as being of a Hearthstone game. It is, however, of the
       Hearthstone menu.

       #+caption: A false positive marked as being a Hearthstone Game frame by linear SVM + Tiny Images
       [[file:figs/tiny_false-positive.png]]

       However, after training on a couple more videos, the I stopped seeing
       false positives. Some false negatives still pop up, but are to be
       expected (for example: I identify the blurred frames at the end of a
       game as /Not Hearthstone/, so when a blurred frame occurs in the middle
       of the game, it gets classified as /Not Hearthstone/ because the blur
       dominates everything else.

*** Amount of Mana
**** Experiment: Contour Method                                                                      :ARCHIVE:
    :PROPERTIES:
    :header-args: :session detect-mana-python :results value file
    :END:

     Load up a known /Hearthstone/ frame. Write it out for reference.

    #+begin_src python
import cv2
import numpy as np
import scipy.misc
import os
from matplotlib import pyplot as plt

cap = cv2.VideoCapture('twitch/Hearthstone/13469865024/c5197696/raws/live_user_sinedd92_1411887437.flv')

ret, frame = cap.read()
frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
orig_path = 'experiments/contours/orig.png'

if not os.path.exists(os.path.dirname(orig_path)):
    os.makedirs(os.path.dirname(orig_path))

scipy.misc.imsave(orig_path, frame)

orig_path
    #+end_src

    #+RESULTS:
    [[file:experiments/contours/orig.png]]

    Split it into roughly the top and bottom halves. Top = opponent, bottom = player
    #+begin_src python
middle = int(frame.shape[0] / 2)
top, bottom = frame[:middle,:,:], frame[middle:,:,:]

top_path = 'experiments/contours/top.png'
bottom_path = 'experiments/contours/bottom.png'
scipy.misc.imsave(top_path, top)
scipy.misc.imsave(bottom_path, bottom)
top_path
    #+end_src

    #+RESULTS:
    [[file:experiments/contours/top.png]]

    #+begin_src python
bottom_path
    #+end_src

    #+RESULTS:
    [[file:experiments/contours/bottom.png]]

    Now find contours...
    #+begin_src python
im = bottom.copy()
im = im.cvtColor(im, cv2.COLOR_RGB2GRAY)
contours, hierarchy = cv2.findContours(im, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
im = bottom.copy()
cv2.drawContours(im, contours, -1, (255, 0, 0))
bc_path = 'experiments/contours/bottom_contours.png'
scipy.misc.imsave(bc_path, im)
bc_path
    #+end_src

    #+RESULTS:
    [[file:experiments/contours/bottom_contours.png]]

    This result clearly sucks.

    Let's try running it on just the blue channel. =bottom= is RGB, so:

    #+begin_src python
im = bottom[:,:,3].copy()
contours, hierarchy = cv2.findContours(im, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
im = bottom.copy()
cv2.drawContours(im, contours, -1, (255, 0, 0))
bc_blue_path = 'experiments/contours/bottom_blue_contours.png'
scipy.misc.imsave(bc_blue_path, im)
bc_blue_path
    #+end_src

    #+RESULTS:
    [[file:experiments/contours/bottom_blue_contours.png]]

    Still a nope. Moving on.
*** Card Detection

    I have chosen to split card detection into two components: cards on the
    board and cards not on the board. The reason for this is that the two
    classes are visually distinct. Cards on the board lose the actual card
    border and display only the ellipsized portrait.

    I may revisit this decision in the future, particularly as I begin training
    the second detector. It may make sense to simply extend the positive set
    rather than constructing a second detector, but I'll cross that bridge
    another day.
**** Constructing the Datasets

     I have located a zip file containing the full versions of the original
     ~700 Hearthstone cards. This forms the basis of my positive set. I may
     consider shears as possible augmentation later (light shearing is common
     in Hearthstone).

     #+name: card-detector:data:positive
     #+begin_src python
class PositiveDataset(luigi.Task):
    zip_path = luigi.Parameter(default='./datasets/card-detector/hearthstone cards.zip')

    # def complete(self):
    #     from zipfile import ZipFile
    #     with ZipFile(self.zip_path) as z:
    #         members = set(z.namelist())
    #     extracted = set([os.path.join(self.output().path, os.path.relpath(dirpath, self.output().path), name)
    #                      for (dirpath, dirnames, filenames) in os.walk(self.output().path)
    #                      for name in filenames])
    #     return members <= extracted

    def output(self):
        return luigi.LocalTarget('./datasets/card-detector/positive')

    def run(self):
        target_dir = self.output().path
        if not os.path.exists(target_dir):
            os.makedirs(target_dir)

        if not os.path.exists(self.zip_path):
            raise Exception("""Please download the hearthstone data from:
http://www.mediafire.com/download/2a25cf1wb1p1acl/hearthstone+cards.zip

Place it in the `datasets/card-detector` directory or specify --zip_path=..""")

        from zipfile import ZipFile
        with ZipFile(self.zip_path) as z:
            z.extractall(target_dir)
     #+end_src

     My negative set consists of random rectangles taken from frames identified
     as being Hearthstone.

     #+name: card-detector:feat:randomrectangles
     #+begin_src python
from hearthstone_classifier import Features
class RandomRectangles(Features):
    # this is an abuse of inheritance. The .npz files created by this task
    # contain nothing. They're effectively marks indicating which videos have
    # been processed.
    count = luigi.IntParameter(default=5)
    min_size = luigi.IntParameter(default=50)
    max_size = luigi.IntParameter(default=600)

    out_dir = './datasets/card-detector/negative'
    def identifier(self):
        return 'random_rectangles'

    def features(self, frame):
        from uuid import uuid4
        from skimage.io import imsave
        sizes = np.ndarray((self.count, 2))
        for i in range(self.count):
            height = np.random.randint(self.min_size, self.max_size)
            width = np.random.randint(self.min_size, self.max_size)

            sizes[i,:] = [height, width]

            frame_size = frame.shape[0:2]
            top_left = (np.random.randint(0, frame_size[0] - width) if frame_size[0] - width > 0 else 0,
                        np.random.randint(0, frame_size[1] - height) if frame_size[1] - height> 0 else 0)
            bottom_right = (min(top_left[0] + width, frame_size[0]),
                            min(top_left[1] + height, frame_size[1]))

            im = frame[top_left[0]:bottom_right[0],
                       top_left[1]:bottom_right[1]
                       :]

            # if not im.shape == (height, width, 3):
            #     print(im.shape)

            filename = uuid4().get_hex() + '.jpg'
            path = os.path.join(self.out_dir, filename)

            imsave(path, im)
        return sizes
     #+end_src


     #+name: card-detector:data:negative
     #+begin_src python
<<card-detector:feat:randomrectangles>>


class NegativeDataset(luigi.Task):

    def requires(self):
        return PositiveDataset(), RandomRectangles(rate=1000)

    def output(self):
        return luigi.LocalTarget('./datasets/card-detector/negative')
     #+end_src

**** DLIB Object Detector
***** Initial Test
      Trying to use DLIB object detection to go from images of cards to a
      cascade classifier.

      #+name: card-detector:dlib-detector:solo
      #+begin_src python
class DLIBDetector_SoloCards(luigi.Task):
    retrain = luigi.BooleanParameter(default=False)

    def complete(self):
        if self.retrain:
            return False
        else:
            return super(self, luigi.Task).complete()

    def requires(self):
        return {'positive': PositiveDataset(), 'negative': NegativeDataset()}

    def output(self):
        return luigi.LocalTarget('training/card-detector.svm')

    def run(self):
        import os
        from glob import glob, iglob
        import dlib
        from skimage import io
        from skimage import transform as tf
        from sklearn.cross_validation import train_test_split

        options = dlib.simple_object_detector_training_options()
        options.C = 5
        options.num_threads = 4
        options.be_verbose = True

        def full_box(im):
            return dlib.rectangle(top=0,
                                  left=0,
                                  right=im.shape[1],
                                  bottom=im.shape[0])
        def top_box(im):
            return dlib.rectangle(top=0,
                                  left=0,
                                  right=im.shape[1],
                                  bottom=im.shape[0]/2)

        def portrait_box(im):
            return dlib.rectangle(top=0,
                                  bottom=242,
                                  left=70,
                                  right=240)

        def imread(path):
            return io.imread(path)[:,:,0:3].copy()

        full = [imread(img_path) for img_path in
                iglob(os.path.join(self.input()['positive'].path, '*.png'))]
        small = [tf.rescale(im, 0.33) for im in full]
        imgs = full + small
        boxes = [[portrait_box(im)] for im in imgs]

        negatives = [imread(img_path) for img_path in
                     glob(os.path.join(self.input()['negative'].path, '*.jpg'))[:1400]]
        negative_boxes = [[] for neg in negatives]

        imgs += negatives
        boxes += negative_boxes

        train_imgs, test_imgs, train_boxes, test_boxes = train_test_split(imgs, boxes, test_size=0.2)
        print(len(train_imgs), len(train_boxes))

        detector = dlib.train_simple_object_detector(train_imgs, train_boxes, options)
        detector.save(self.output().path)

        print("Accuracy: {0}".format(dlib.test_simple_object_detector(test_imgs, test_boxes, detector)))
     #+end_src

      Quick test script to see if we're getting detections.

      #+name: card-detector:eval:dlib-detector
      #+begin_src python :tangle dlib-eval.py
from __future__ import print_function
import dlib
from skimage import io
from learn2play.card_detector import DLIBDetector_SoloCards
from glob import iglob

test_ims = iglob('./datasets/card-detector/real/*.jpg')

det = dlib.simple_object_detector(DLIBDetector_SoloCards().output().path)

win = dlib.image_window()
win.set_image(det)

raw_input()

for path in test_ims:
    im = io.imread(path)
    dets = det(im)
    print(len(dets))

    win.clear_overlay()
    win.set_image(im)
    win.add_overlay(dets)

    if len(dets) > 0:
        raw_input()
     #+end_src

      Nope.

***** Round 2: =imglab=
      Trying to label real images with dlib's =imglab= tool. Labels are:

      - card
      - minion
      - hero
      - mana_counter
      - hero_power
      - card_back
      - weapon

      After some testing it appears that I have to annotate them all in
      *separate* XML files (because they have different aspect ratios. No big
      deal, but annoying.

***** +Card+ Minion Detection

      My minion bounding boxes all have consistent aspect ratios, so I'm going
      to try those first instead of cards.

      #+name: card-detector:dlib-detector:minion
      #+begin_src python
class MinionLabels(luigi.task.ExternalTask):

    def output(self):
        return {'train': luigi.LocalTarget('datasets/detector/minions/train.xml'),
                'test': luigi.LocalTarget('datasets/detector/minions/test.xml')}

class DLIBDetector_Minion(luigi.Task):
    retrain = luigi.BooleanParameter(default=False)

    def complete(self):
        if self.retrain:
            return False
        else:
            return super(self, luigi.Task).complete()

    def requires(self):
        return MinionLabels()

    def output(self):
        return luigi.LocalTarget('training/minion-detector.svm')

    def run(self):
        import os
        from glob import glob, iglob
        import dlib
        from skimage import io
        from skimage import transform as tf
        from sklearn.cross_validation import train_test_split

        options = dlib.simple_object_detector_training_options()
        options.C = 5
        options.num_threads = 4
        options.be_verbose = True

        dlib.train_simple_object_detector(self.input()['train'].path, self.output().path, options)

        print("Accuracy: {0}".format(dlib.test_simple_object_detector(
            self.input()['test'].path,
            self.output().path
        )))
     #+end_src

      #+name: card-detector:eval:dlib-detector:minion
      #+begin_src python :tangle dlib-eval-minion.py
from __future__ import print_function
import dlib
from skimage import io
from learn2play.card_detector import DLIBDetector_Minion
from glob import iglob

test_ims = iglob('./datasets/card-detector/real/*.jpg')

det = dlib.simple_object_detector(DLIBDetector_Minion().output().path)

win = dlib.image_window()
win.set_title('Minion Detection')
win.set_image(det)

raw_input()

for path in test_ims:
    im = io.imread(path)
    dets = det(im)
    print(len(dets))

    win.clear_overlay()
    win.set_image(im)
    win.add_overlay(dets)

    if len(dets) > 0:
        raw_input()
     #+end_src

      Great success!

      #+caption: Minion detection results on an unseen screencap.
      [[file:figs/minion-detection.png]]

      I have run into a couple of failure cases, but believe that extending the
      training set (this set is a mere *10 frames!*) will resolve this.

      The most *important* failure case is shown here in the middle of the
      player's board: less than all of the minion is contained within the
      bounding box. If this occurs /consistently/ (ie ∀ a,b instances of the
      minion M: box(a) ≡ box(b)), then it is not a problem. If not, then it
      could cause classification problems. Probably best to try to avoid it.

      Besides, it looks ugly.


****** Post-Retrain

       Retraining on more carefully taken data did not improve boxes. This
       could be problematic. Will have to wait and see.

**** Putting it Together

     #+begin_src python :tangle learn2play/card_detector.py :comments both :noweb yes
import luigi
import cPickle as pickle
import cv2
import sklearn
import numpy as np
import random
from matplotlib import pyplot as plt
import os
import sys
from glob import glob, iglob

<<card-detector:data:positive>>


<<card-detector:data:negative>>


<<card-detector:dlib-detector:solo>>


<<card-detector:dlib-detector:minion>>

if __name__ == "__main__":
    luigi.run(local_scheduler=True)
     #+end_src


*** +Card+ Minion Classification

    Due to time constraints, I'm using my minion detector instead of a
    hypothetical card detector.

    Soooo lets try some features!

**** SURF/SIFT

     Not sure exactly how SURF features will work on this, but what the
     hell. Let's try!

     #+name: minion-classifier:test:surf
     #+BEGIN_SRC python :tangle describe-minions.py
from __future__ import print_function
import cv2
import dlib
from skimage import io
from learn2play.card_detector import DLIBDetector_Minion
import argparse
import itertools as it
import numpy as np

det = dlib.simple_object_detector(DLIBDetector_Minion().output().path)

parser = argparse.ArgumentParser()
parser.add_argument('images', nargs='*')

args = parser.parse_args()

sift = cv2.SIFT()
flann = cv2.FlannBasedMatcher({"algorithm": 0, "trees": 5}, {"checks": 50})

for img_path in args.images:
    im = io.imread(img_path)
    dets = det(im)

    print(len(dets))

    minions = [im[d.top():d.bottom(), d.left():d.right(), :]
               for d in dets]
    descriptors = [sift.detectAndCompute(m, None)[1] for m in minions]

    matches = [((c[0][0], c[1][0]),
               [(m, n) for (m, n) in flann.knnMatch(c[0][1], c[1][1], k=2)
                if m.distance < 0.7 * n.distance])
               for c in it.combinations(zip(minions, descriptors), 2)]

    for m in matches:
        print(len(m[1]))
        if len(m[1]) > 30:
            io.imsave('1.jpg', m[0][0])
            io.imsave('2.jpg', m[0][1])
     #+END_SRC

     SURF sucks for this it seems. With low hessian threshold (for lots of keypoints),
     very few keypoint matches occur. With a high hessian threshold, very few
     matches occur.

     SIFT does better: setting the match threshold to 30 consistently detects
     matching minions. Time to make proper Luigi tasks and make some plots.

     | [[./figs/minion_a.jpg]] | [[./figs/minion_b.jpg]] | [[./figs/minion_a2.jpg]] | [[./figs/minion_b2.jpg]] | [[./figs/minion_a3.jpg]] | [[./figs/minion_b3.jpg]] |

** Random Utils
*** OpenCV Key Printer

    #+begin_src python :tangle keyprint.py
import cv2
import numpy as np

im = np.zeros((256,256))
cv2.imshow('frame', im)
while True:
    key = cv2.waitKey() & 0xFF
    print(key)
    if key == 27:
        break

cv2.destroyAllWindows()
    #+end_src
*** Frame Extractor

    Quick utility to pull frames out of a video at a given sample rate.

    Usage:
    #+BEGIN_EXAMPLE
    # python utils/extract-frames.py rate video [video...]
    python utils/extract-frames.py 1000 twitch/Hearthstone/*/*/raws/*.flv
    #+END_EXAMPLE

    #+name: extract-frames
    #+begin_src python :tangle utils/extract-frames.py :noweb yes
from __future__ import print_function
import cv2
from skimage import io
import os
from hashlib import sha256
import sys


<<meta>>


<<seek>>


def frames(cap, rate=1):
    index = current_frame(cap)

    while cap.isOpened():
        if rate > 1 and index > 0:
            # don't need to seek if we're going frame by frame
            # adding extra seeks is surprisingly slow
            try:
                seek(cap, index)
            except:
                break

        ret, frame = cap.read()

        if ret:
            yield index, frame
        else:
            break

        # note: cap.read() advances the current frame by 1
        index += rate

def extract_frames(rate, path, root='.'):
    cap = cv2.VideoCapture(path)

    fmt = os.path.join(root, "{hash}_{name}_{idx}.jpg")

    name = os.path.basename
    hasher = lambda p: sha256(p).hexdigest()

    for idx, frame in frames(cap, rate):
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        io.imsave(fmt.format(hash=hasher(path),
                             name=name(path),
                             idx=idx),
                  frame)

    cap.release()

def main():
    import argparse
    parser = argparse.ArgumentParser("Quick utility to extract frames from videos")
    parser.add_argument("--root", type=str, default=".", help="Root folder to extract frames to")
    parser.add_argument("rate", type=int,
                        help="Rate at which to take frames. If n = rate, every nth frame is taken")
    parser.add_argument("videos", nargs=argparse.REMAINDER)

    args = parser.parse_args()

    for path in args.videos:
        print("Extracting frames from {0}...".format(path), file=sys.stderr)
        extract_frames(args.rate, path, args.root)

if __name__ == "__main__":
    main()
    #+end_src
*** Predict Hearthstone

    This script is based off of the output of [[Hearthstone or Not?]]. It returns 0
    if the frame is /Hearthstone/, -1 otherwise.

    #+name: utils:predict-hearthstone
    #+BEGIN_SRC python :tangle utils/predict-hearthstone.py
from __future__ import print_function
from sklearn.externals import joblib
from skimage.io import imread
from scipy.misc import imresize
import os
import argparse
from sys import exit

parser = argparse.ArgumentParser()
parser.add_argument('--classifier', default=os.path.join(os.path.dirname(__file__), '../training/hearthstone_model_tiny_16x16.jl'))
parser.add_argument('image')

args = parser.parse_args()

im = imread(args.image)
im = im[:,:, ::-1]
classifier = joblib.load(args.classifier)

tiny = imresize(im, (16, 16)).reshape(-1)
score = classifier.predict([tiny])

if score[0] == 1:
    exit(0)
else:
    exit(score[0])
    #+END_SRC
