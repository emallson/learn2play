* L2P Hearthstone
** Part 1
*** Hearthstone or Not?

    The objective is to take videos and classify whether they are Hearthstone
    or not.

**** Annotating
     :PROPERTIES:
     :header-args: :tangle hs_annotate.py :comments both :noweb yes
     :END:

     The first step is necessarily annotation. I have a bunch of videos and
     need to note the exact frames at which the transition from Hearthstone to
     Not (or visa-versa) occurs.


***** Annotation Tool
      This is a tool to annotate videos with binary labels.

      #+begin_src python
import numpy as np
import cv2
import sys
from functools import partial
     #+end_src

      #+name: load_video
      #+begin_src python
def load_video(filename):
    cap = cv2.VideoCapture(filename)
    return cap, current_frame(cap), num_frames(cap)
     #+end_src

      #+name: meta
      #+begin_src python
def num_frames(cap):
    return int(cap.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT))

def current_frame(cap):
    return int(cap.get(cv2.cv.CV_CAP_PROP_POS_FRAMES))
     #+end_src

      #+name: seek
      #+begin_src python
def seek(cap, pos):
    if pos > num_frames(cap):
        raise Exception

    cap.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, pos)
     #+end_src

      #+begin_src python
def main():
    cap, position, max_position = load_video(sys.argv[1])

    window_name = 'frame'  # winning an award for 'most creative name' for sure
    cv2.namedWindow(window_name)
    cv2.createTrackbar('frame_bar', window_name,
                       position, max_position,
                       partial(seek, cap))

    partition_points = []
    advance = False
    save = True

    ret, frame = cap.read()

    try:
        while cap.isOpened() and current_frame(cap) < num_frames(cap):
            if advance:
                ret, frame = cap.read()
                cv2.setTrackbarPos('frame_bar', window_name, current_frame(cap))

            cv2.imshow(window_name, frame)

            key = cv2.waitKey(25) & 0xFF

            if key == 27:  # ESC
                save = False
                break
            elif key == ord('.'):
                ret, frame = cap.read()
                cv2.setTrackbarPos('frame_bar', window_name, current_frame(cap))
            elif not advance and key == ord('t'):
                index = current_frame(cap)
                if index in partition_points:
                    partition_points.remove(index)
                else:
                    partition_points.append(index)
                    partition_points.sort()
                    print(partition_points)
            elif key == ord(' '):
                advance = not advance

        cap.release()
        cv2.destroyAllWindows()

    except Exception as e:
        print(e)

    finally:
        if save:
            with open(sys.argv[1] + '.partitions', 'w') as dest:
                dest.write(','.join(map(str, partition_points)))

if __name__ == "__main__":
    main()
     #+end_src
**** Classifying

***** Extracting Frames

      Gonna use a generator for frames from a video because =cv2= doesn't have
      a nice iterable for them.

      #+name: frame-gen
      #+begin_src python
def frames(cap):
    while True:
        ret, frame = cap.read()
        if ret:
            yield frame
        else:
            break
      #+end_src

      This function creates the cv2 cap object as well as the list of partition points.
      #+name: cap-open
      #+begin_src python
def cap_open(path):
    with open(path + '.partitions') as f:
        return cv2.VideoCapture(path), f.read().split(',')
      #+end_src

***** Extracting Features
****** Base Class

       I want to be able to easily add new features. Building a base Luigi task
       for computing features makes sense. The contract for extending this class is
       very simple:

       - =identifier(self) -> str=
         Generates a string identifying the type of features being computed.
       - =features(self, frame) -> np.array=
         Generates a set of features for a frame.

       #+name: task:features
       #+begin_src python
class Features(luigi.Task):
    video = luigi.Parameter()
    rate = luigi.IntParameter(default=10)  # 1 in $rate frames are used
    color = luigi.BooleanParameter(default=True)

    def identifier(self):
        raise ValueError("Do not use the base class!")

    def output(self, type=None):
        path = self.video + '_{type}_{gray}{rate}.npz'.format(
            rate=self.rate,
            type=self.identifier(),
            gray=('gray_' if not self.color else '')
        )
        return luigi.LocalTarget(path)

    def features(self):
        raise ValueError("Do not use the base class!")

    def run(self):
        <<frame-gen>>

        cap = cv2.VideoCapture(self.video)

        video_features = {str(index): self.features(frame)
                          for index, frame in enumerate(frames(cap))
                          if index % self.rate == 0}
        # some frames produce None for their descriptor, don't know why
        video_features = {k: v for k, v in video_features.items()
                          if v is not None}

        with self.output().open('w') as out:
            np.savez_compressed(out, **video_features)
       #+end_src

****** Specifying a Feature Type

       To specify the kind of features for a task in a variable fashion, simply
       add a =TaskParameter()=, which will return a sub-class of =Features=
       or throw an error.

       #+name: param:features
       #+begin_src python
class TaskParameter(luigi.Parameter):
    def parse(self, x):
        if type(x) == str:
            cls = globals()[x]
        else:
            cls = x

        if issubclass(cls, luigi.Task) and not cls == luigi.Task:
            return cls
        else:
            raise ValueError('Unknown task requested.')
      #+end_src

****** SURF Features
       SURF feature implementation using =cv2.SURF=.

       #+name: features:surf
       #+begin_src python
class SURFFeatures(Features):
    hessianThreshold = luigi.IntParameter(default=5000)
    upright = luigi.BooleanParameter(default=True)

    def identifier(self):
        return 'surf_{upright}{hessian}'.format(
            upright='upright_' if self.upright else '',
            hessian=self.hessianThreshold
        )

    def surf(self):
        return cv2.SURF(upright=self.upright,
                        hessianThreshold=self.hessianThreshold)

    def features(self, frame):
        kp, desc = self.surf().detectAndCompute(frame, None)
        return desc
       #+end_src

****** SURF Histograms

       SURF BoVW histograms. Has a forward dependency on the =Vocabulary= task.

       #+name: features:surf_histogram
       #+begin_src python
class SURFHistograms(SURFFeatures):

    def requires(self):
        return {
            'vocab': Vocabulary(
                rate=self.rate,
                features=SURFFeatures
            ),
            'surf': SURFFeatures(
                rate=self.rate,
                video=self.video
            )
        }

    def identifier(self):
        return 'surf_hist_{upright}{hessian}'.format(
            upright='upright_' if self.upright else '',
            hessian=self.hessianThreshold
        )

    def run(self):
        centers = np.load(self.input()['vocab'].path)['centers']
        features = np.load(self.input()['surf'].path)

        def histogram(feat):
            h = np.zeros((len(centers),), dtype='float32')
            for f in feat:
                dist = np.linalg.norm(centers - f, axis=1)
                h[dist.argmax()] += 1
            return h / np.linalg.norm(h)

        with self.output().open('w') as f:
            np.savez_compressed(
                f,
                ,**{k: histogram(v)
                   for k, v in features.iteritems()})
       #+end_src
****** Tiny Images

       Simple Tiny Images implementation: resize the image to be really
       fragging small.

       #+name: features:tiny
       #+begin_src python
class TinyFeatures(Features):
    size = luigi.Parameter(default='16x16')

    def identifier(self):
        return 'tiny_{size}'.format(size=self.size)

    def features(self, frame):
        from scipy.misc import imresize

        size = map(int, self.size.split('x'))
        return imresize(frame, size).reshape(-1)
      #+end_src
****** All Features Combined

       #+name: all_features
       #+begin_src python
<<task:features>>


<<param:features>>


<<task:vocabulary>>


<<features:surf>>


<<features:surf_histogram>>


<<features:tiny>>
       #+end_src

***** Building Vocabulary

      I am using a Bag of Visual Words model with k-means clustering to develop
      a vocabulary. The =Vocabulary= task encodes the construction of a
      vocabulary on a set of videos with a number of words.

      #+name: task:vocabulary
      #+begin_src python
class Vocabulary(luigi.Task):
    num_words = luigi.IntParameter(default=300)
    num_videos = luigi.IntParameter(default=10)
    rate = luigi.IntParameter(default=100)
    seed = luigi.IntParameter(default=None, significant=False)
    features = TaskParameter()

    def output(self):
        path = 'training/{ident}_{words}_{videos}.npz'.format(
            ident=self.features(video='', rate=self.rate).identifier(),
            words=self.num_words,
            videos=self.num_videos
        )
        return luigi.LocalTarget(path)

    def run(self):
        from glob import glob
        from random import seed, shuffle
        paths = glob('twitch/Hearthstone/*/*/raws/*.flv')
        seed(self.seed)
        shuffle(paths)

        all_features = []
        feats = yield [self.features(video=path,rate=self.rate)
                       for path in paths[:self.num_videos]]

        for feat in feats:
            with feat.open('r') as f:
                features = dict(np.load(f).items()).values()
                all_features += features

        all_features = np.vstack(all_features)

        compactness, labels, centers = cv2.kmeans(
            all_features, self.num_words,
            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,1000,1.0),
            10, 0)

        with self.output().open('w') as out:
            np.savez_compressed(
                out,
                compactness=compactness,
                labels=labels,
                centers=centers
            )
      #+end_src

***** Constructing the SVM Model

      #+name: task:svm-hs?
      #+begin_src python

class Dataset(luigi.Task):
    rate = luigi.IntParameter(default=100)
    features = TaskParameter()

    def known_paths(self):
        return glob('twitch/Hearthstone/*/*/raws/*.flv.partitions')

    def response(self, partitions, frame_num):
        hearthstone = False
        for partition in partitions:
            if frame_num < partition:
                return (1 if hearthstone else -1)
            hearthstone = not hearthstone
        return (1 if hearthstone else -1)


    def load_train_test(self, paths, features):
        trainData = []
        responses = []
        for path, feat in zip(paths, features):
            with open(path, 'r') as f:
                p_raw = f.read()
                if p_raw:
                    partitions = [int(p) for p in p_raw.split(',')]
                else:
                    partitions = []

            with feat.open('r') as f:
                features = np.load(f).items()
                for frame_num, feat in features:
                    frame_num = int(frame_num)
                    resp = self.response(partitions, frame_num)
                    trainData.append(feat)
                    responses.append(resp)

        trainData = np.vstack(trainData)
        responses = np.vstack(responses)
        return trainData, responses

    def requires(self):
        paths = self.known_paths()
        return [self.features(video=path.rstrip('.partitions'),
                              rate=self.rate)
                for path in paths]

    def output(self):
        path = 'datasets/{id}_{rate}.npz'.format(
            id=self.features(video='', rate=self.rate).identifier(),
            rate=self.rate
        )
        return luigi.LocalTarget(path)

    def run(self):
        train, resp = self.load_train_test(self.known_paths(), self.input())

        with self.output().open('w') as f:
            np.savez_compressed(f,
                                trainData=train,
                                responses=resp)

class HearthstoneModel(luigi.Task):
    rate = luigi.IntParameter(default=100)
    features = TaskParameter()
    rand_state = luigi.IntParameter(default=None)
    test_size = luigi.FloatParameter(default=0.5)

    def __init__(self, *args, **kwargs):
        from random import randint
        super(HearthstoneModel, self).__init__(*args, **kwargs)
        if self.rand_state is None:
            self.rand_state = random.randint(0, np.iinfo(np.uint32).max)

    def requires(self):
        return Dataset(rate=self.rate, features=self.features)

    def output(self):
        return luigi.LocalTarget('training/hearthstone_model_{ident}.jl'.format(
            ident=self.features(video='', rate=self.rate).identifier()
        ))

    def run(self):
        from sklearn import svm
        from sklearn.cross_validation import train_test_split
        from sklearn.externals import joblib
        dataset = np.load(self.input().path)

        f_train, _, r_train, _ = train_test_split(
            dataset['trainData'], dataset['responses'],
            test_size=self.test_size,
            random_state=self.rand_state
        )

        clf = svm.LinearSVC()
        clf.fit(
            dataset['trainData'],
            dataset['responses'],
        )

        joblib.dump(clf, self.output().path)
      #+end_src
***** Evaluation
      An ROC curve makes sense for evaluating the performance of this
      method. The following task computes one, producing a PNG plot with the
      AUC on it.

      #+name: task:roc
      #+begin_src python
class ROCCurve(luigi.Task):
    model = TaskParameter()
    rate = luigi.IntParameter()
    features = TaskParameter()
    rand_state = luigi.IntParameter(default=None)
    test_size = luigi.FloatParameter(default=0.5)

    def __init__(self, *args, **kwargs):
        from random import randint
        super(ROCCurve, self).__init__(*args, **kwargs)
        if self.rand_state is None:
            self.rand_state = random.randint(0, np.iinfo(np.uint32).max)

    def requires(self):
        return {
            'model': self.model(
                rand_state=self.rand_state,
                test_size=self.test_size,
                features=self.features,
                rate=self.rate
            ),
            'dataset': Dataset(rate=self.rate,
                               features=self.features)
        }

    def output(self):
        self.model_name = os.path.splitext(
            os.path.basename(self.input()['model'].path))[0]

        return luigi.LocalTarget('plots/roc_{rate}_{model_name}.png'.format(
            model_name=self.model_name,
            rate=self.rate
        ))

    def run(self):
        from sklearn.cross_validation import train_test_split
        from sklearn.externals import joblib
        from sklearn.metrics import roc_curve, auc

        dataset = np.load(self.input()['dataset'].path)

        _, f_test, _, r_test = train_test_split(
            dataset['trainData'], dataset['responses'],
            test_size=self.test_size,
            random_state=self.rand_state
        )

        classifier = joblib.load(self.input()['model'].path)

        r_scores = classifier.predict(f_test)

        fpr, tpr, _ = roc_curve(r_test, r_scores)
        auc = auc(fpr, tpr)

        plt.figure()
        plt.plot(fpr, tpr)
        plt.xlabel('False Positive Rate')
        plt.xlim([0.0, 1.0])
        plt.ylabel('True Positive Rate')
        plt.ylim([0.0, 1.05])
        plt.title("ROC for {model_name} (Test size: {ts}, AUC: {auc})".format(
            model_name=self.model_name,
            ts=self.test_size,
            auc=auc
        ))
        try:
            os.makedirs(os.path.dirname(self.output().path))
        except:
            pass

        plt.savefig(self.output().path)
      #+end_src

***** Putting it Together

      #+begin_src python :tangle learn2play.py :comments both :noweb yes
import luigi
import cPickle as pickle
import cv2
import sklearn
import numpy as np
import random
from matplotlib import pyplot as plt
import os
import sys
from glob import glob, iglob


<<all_features>>


<<task:svm-hs?>>

<<task:roc>>

if __name__ == "__main__":
    luigi.run(local_scheduler=True)
      #+end_src

***** Results

      Results are computed as the AUC of ROC curves with a 50/50 train/test
      split. Only 6 videos were annotated.

****** SURF Histogram Features
       These features are computed with =hessianThreshold=5000=, a Bag of
       Visual Words with =n=200= and a sampling rate of =1/1000=.

       #+name: roc:svm+surf
       #+begin_src python :results value file :exports results
import luigi
import sys
from luigi.scheduler import CentralPlannerScheduler as CSP
from luigi.worker import Worker
from learn2play import ROCCurve, HearthstoneModel, SURFHistograms

roc = ROCCurve(
    rate=1000,
    model=HearthstoneModel,
    features=SURFHistograms
)

w = Worker(scheduler=CSP())
w.add(roc)
if w.run():
    return roc.output().path
       #+end_src

       #+RESULTS: roc:svm+surf
       [[file:plots/roc_1000_hearthstone_model_surf_hist_upright_5000.png]]

       Not as good as I'd hoped, especially given how long it takes to compute
       SURF Features over that many frames.

       *AUC:* 0.846

****** Tiny Images

       Tiny Images seem like a good fit for Hearthstone games, given that
       rotation and position within the image are pretty constant.

       #+name: roc:svm+tiny
       #+begin_src python :results value file :exports results
import luigi
import sys
from luigi.scheduler import CentralPlannerScheduler as CSP
from luigi.worker import Worker
from learn2play import ROCCurve, HearthstoneModel, TinyFeatures

roc = ROCCurve(
    rate=100,
    model=HearthstoneModel,
    features=TinyFeatures
)

w = Worker(scheduler=CSP())
w.add(roc)
if w.run():
    return roc.output().path
       #+end_src

       #+RESULTS: roc:svm+tiny
       [[file:plots/roc_100_hearthstone_model_tiny_16x16.png]]

       Turns out that that intuition was spot-on.

       *AUC*: 1.0

****** Secondary Evaluation

       Since the Tiny Images feature set performed so incredibly well (too
       well, some might say), I decided to give it a secondary semi-manual
       evaluation. Here is a small script which displays a video with an
       overlay of whether it is =Hearthstone= or =Not Hearthstone=.
       #+begin_src python :tangle part1_eval.py :noweb yes :exports none
import cv2
from learn2play import *
from sys import argv
from sklearn.externals import joblib

<<frame-gen>>

model_task = HearthstoneModel(rate=1000, features=TinyFeatures)
classifier = joblib.load(model_task.output().path)

tinyfy = TinyFeatures(video='', rate=1000).features

cap = cv2.VideoCapture(argv[1])

def textify(frame, text):
    copy = frame.copy()
    cv2.putText(copy, text, (10,100), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)
    return copy

cv2.namedWindow('frame')
for frame in frames(cap):
    print frame.shape
    tiny = tinyfy(frame)
    prediction = classifier.predict([tiny])
    print prediction
    if prediction == 1:
        cv2.imshow('frame', textify(frame, 'Hearthstone'))
    else:
        cv2.imshow('frame', textify(frame, 'Not Hearthstone'))
    cv2.waitKey(10)
cv2.destroyAllWindows()
       #+end_src

       #+begin_src sh
       python part1_eval.py twitch/Hearthstone/13470812000/c3673800/raws/live_user_morikcm_1391579178.flv
       #+end_src

       This secondary evaluation revealed that dataset bias is a factor in the
       accuracy of tiny images. For example, the following frame is (wrongly)
       marked as being of a Hearthstone game. It is, however, of the
       Hearthstone menu.

       #+caption: A false positive marked as being a Hearthstone Game frame by linear SVM + Tiny Images
       [[file:figs/tiny_false-positive.png]]

       However, after training on a couple more videos, the I stopped seeing
       false positives. Some false negatives still pop up, but are to be
       expected (for example: I identify the blurred frames at the end of a
       game as /Not Hearthstone/, so when a blurred frame occurs in the middle
       of the game, it gets classified as /Not Hearthstone/ because the blur
       dominates everything else.

** Random Utils
*** OpenCV Key Printer

    #+begin_src python :tangle keyprint.py
import cv2
import numpy as np

im = np.zeros((256,256))
cv2.imshow('frame', im)
while True:
    key = cv2.waitKey() & 0xFF
    print(key)
    if key == 27:
        break

cv2.destroyAllWindows()
    #+end_src
